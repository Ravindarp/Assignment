{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b4690",
   "metadata": {},
   "source": [
    "A contingency matrix, also known as a confusion matrix, is a table that is used to summarize the performance of a classification model. It shows the number of data points that were correctly classified and the number of data points that were misclassified.\n",
    "\n",
    "The contingency matrix is divided into four quadrants:\n",
    "\n",
    "True Positive (TP): This quadrant represents the number of data points that were actually positive and were correctly classified as positive by the model.\n",
    "True Negative (TN): This quadrant represents the number of data points that were actually negative and were correctly classified as negative by the model.\n",
    "False Positive (FP): This quadrant represents the number of data points that were actually negative but were incorrectly classified as positive by the model.\n",
    "False Negative (FN): This quadrant represents the number of data points that were actually positive but were incorrectly classified as negative by the model.\n",
    "The contingency matrix can be used to calculate a variety of metrics to evaluate the performance of a classification model, such as accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43469eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6053cc",
   "metadata": {},
   "source": [
    "A pair confusion matrix is a special type of confusion matrix that is used to evaluate the performance of a clustering algorithm. It is similar to a regular confusion matrix, but it only considers the pairs of clusters that are most similar to each other.\n",
    "The pair confusion matrix can be used to calculate a variety of metrics to evaluate the performance of a clustering algorithm, such as purity, completeness, and separation.\n",
    "\n",
    "Purity: Purity is the percentage of data points that were correctly clustered together. It is calculated by dividing the number of true positives by the total number of data points.\n",
    "Purity = TP / (TP + FP + FN)\n",
    "Completeness: Completeness is the percentage of data points that were actually in the same cluster that were correctly clustered together. It is calculated by dividing the number of true positives by the total number of data points that were actually in the same cluster.\n",
    "Completeness = TP / (TP + FN)\n",
    "Separation: Separation is the percentage of data points that were actually in different clusters that were correctly clustered in different clusters. It is calculated by dividing the number of true negatives by the total number of data points that were actually in different clusters.\n",
    "Separation = TN / (TN + FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cca329",
   "metadata": {},
   "source": [
    "\n",
    "An extrinsic measure in the context of natural language processing (NLP) is a measure of the performance of a language model that is based on its ability to perform a specific task. For example, an extrinsic measure of the performance of a machine translation model could be the BLEU score, which measures the similarity between the translated text and the human-translated text.\n",
    "\n",
    "Extrinsic measures are typically used to evaluate the performance of language models because they are more realistic than intrinsic measures. Intrinsic measures are measures of the performance of a language model that are based on its internal properties, such as the accuracy of its predictions. However, intrinsic measures are often not very reliable because they do not take into account the real-world tasks that language models are used for.\n",
    "\n",
    "Extrinsic measures are typically used in conjunction with intrinsic measures to evaluate the performance of language models. The intrinsic measures can be used to get an idea of the overall performance of the language model, while the extrinsic measures can be used to get an idea of how well the language model performs on specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e1774",
   "metadata": {},
   "source": [
    "An intrinsic measure in the context of machine learning is a measure of the performance of a machine learning model that is based on its internal properties, such as the accuracy of its predictions. For example, an intrinsic measure of the performance of a linear regression model could be the mean squared error (MSE), which measures the average squared difference between the predicted values and the actual values.\n",
    "\n",
    "An extrinsic measure in the context of machine learning is a measure of the performance of a machine learning model that is based on its ability to perform a specific task. For example, an extrinsic measure of the performance of a spam filter could be the accuracy of its predictions, which is the percentage of emails that are correctly classified as spam or ham.\n",
    "\n",
    "The main difference between intrinsic and extrinsic measures is that intrinsic measures are based on the internal properties of the machine learning model, while extrinsic measures are based on the ability of the machine learning model to perform a specific task.\n",
    "\n",
    "Intrinsic measures are often used as a starting point for evaluating the performance of a machine learning model. They can be used to get an idea of how well the model is performing overall. However, intrinsic measures are not always reliable because they do not take into account the specific task that the model is being evaluated for.\n",
    "\n",
    "Extrinsic measures are often used to evaluate the performance of a machine learning model after it has been fine-tuned for a specific task. They can be used to get an idea of how well the model performs on that specific task. However, extrinsic measures can be difficult to obtain because they require a labeled dataset of the task that the model is being evaluated for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d681c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524673ad",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is used to summarize the performance of a classification model. It shows the number of data points that were correctly classified and the number of data points that were misclassified.\n",
    "\n",
    "The confusion matrix is divided into four quadrants:\n",
    "\n",
    "True Positive (TP): This quadrant represents the number of data points that were actually positive and were correctly classified as positive by the model.\n",
    "True Negative (TN): This quadrant represents the number of data points that were actually negative and were correctly classified as negative by the model.\n",
    "False Positive (FP): This quadrant represents the number of data points that were actually negative but were incorrectly classified as positive by the model.\n",
    "False Negative (FN): This quadrant represents the number of data points that were actually positive but were incorrectly classified as negative by the model.\n",
    "The confusion matrix can be used to identify the strengths and weaknesses of a model by looking at the number of data points in each quadrant.\n",
    "The number of true positives (TP): This represents the number of data points that were correctly classified. A high number of true positives indicates that the model is good at identifying positive data points.\n",
    "The number of true negatives (TN): This represents the number of data points that were correctly classified. A high number of true negatives indicates that the model is good at identifying negative data points.\n",
    "The number of false positives (FP): This represents the number of data points that were incorrectly classified as positive. A high number of false positives indicates that the model is making too many mistakes.\n",
    "The number of false negatives (FN): This represents the number of data points that were incorrectly classified as negative. A high number of false negatives indicates that the model is missing too many data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfef3c0",
   "metadata": {},
   "source": [
    "here are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms:\n",
    "\n",
    "Silhouette coefficient: The silhouette coefficient is a measure of how well each data point is clustered. It is calculated by taking the average of the difference between the distance of a data point to its own cluster and the distance of the data point to the nearest cluster. A higher silhouette coefficient indicates that the data point is well-clustered.\n",
    "Calinski-Harabasz index: The Calinski-Harabasz index is a measure of the separation between clusters. It is calculated by taking the ratio of the between-cluster variance to the within-cluster variance. A higher Calinski-Harabasz index indicates that the clusters are well-separated.\n",
    "Davies-Bouldin index: The Davies-Bouldin index is a measure of the compactness and separation of clusters. It is calculated by taking the average of the ratio of the within-cluster scatter to the between-cluster separation for each cluster. A lower Davies-Bouldin index indicates that the clusters are well-compacted and well-separated.\n",
    "Hubert index: The Hubert index is a measure of the separation and compactness of clusters. It is calculated by taking the average of the between-cluster distance and the within-cluster distance. A higher Hubert index indicates that the clusters are well-separated and well-compacted.\n",
    "Gap statistic: The gap statistic is a measure of the difference between the within-cluster variance of the data and the within-cluster variance of a random data set. A larger gap statistic indicates that the data is more likely to be clustered than a random data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd6f9e",
   "metadata": {},
   "source": [
    "here are some limitations of using accuracy as a sole evaluation metric for classification tasks:\n",
    "\n",
    "Accuracy can be misleading in cases of imbalanced classes. This is when one class is much more common than the others. In this case, an algorithm can achieve a high accuracy simply by predicting the majority class for all data points.\n",
    "Accuracy does not take into account the misclassification of rare classes. This can be a problem if the rare classes are important or if the cost of misclassifying them is high.\n",
    "Accuracy does not take into account the different types of errors. For example, a false positive is when an algorithm predicts that a data point belongs to a class when it does not, while a false negative is when an algorithm predicts that a data point does not belong to a class when it does. Accuracy does not distinguish between these two types of errors, even though they can have different consequences.\n",
    "Here are some ways to address the limitations of using accuracy as a sole evaluation metric for classification tasks:\n",
    "\n",
    "Use other evaluation metrics in addition to accuracy. Some other common evaluation metrics include precision, recall, and F1 score. Precision measures the fraction of data points that are correctly classified as positive, while recall measures the fraction of positive data points that are correctly classified. F1 score is a combination of precision and recall.\n",
    "Use stratified sampling to balance the classes. This means that the data set is divided into subsets so that each subset has the same proportion of data points from each class. This can help to prevent the algorithm from simply predicting the majority class for all data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
