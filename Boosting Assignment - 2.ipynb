{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca77fb1",
   "metadata": {},
   "source": [
    "\n",
    "Gradient boosting regression is a machine learning technique that uses an ensemble of weak learners to create a strong learner. The weak learners are typically decision trees, and they are added sequentially to the model, each one trying to correct the errors of the previous models.\n",
    "\n",
    "The gradient boosting algorithm works by iteratively fitting a new model to the residual errors of the previous model. The residual errors are the differences between the predicted values and the actual values. The new model is fit using a technique called gradient descent, which minimizes the loss function.\n",
    "\n",
    "The loss function is a measure of how well the model fits the data. The most common loss function for regression is the mean squared error (MSE). The MSE is the sum of the squared differences between the predicted values and the actual values.\n",
    "\n",
    "Gradient boosting regression is a powerful machine learning technique that can be used to solve a variety of regression problems. It is particularly well-suited for problems where the relationship between the independent and dependent variables is nonlinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(y.shape, np.mean(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "                        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "                        y_pred += self.learning_rate * tree.predict(X)\n",
    "                        self.estimators.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], np.mean(y))\n",
    "                for tree in self.estimators:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([2, 4, 5, 4, 5])\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "X_test = np.array([[6], [7], [8]])\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_train, gb_regressor.predict(X_train))\n",
    "r2 = r_squared(y_train, gb_regressor.predict(X_train))\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_mse = mean_squared_error(y_train, best_estimator.predict(X_train))\n",
    "best_r2 = r_squared(y_train, best_estimator.predict(X_train))\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Mean Squared Error:\", best_mse)\n",
    "print(\"Best R-squared:\", best_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a053332",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3701098",
   "metadata": {},
   "source": [
    "A weak learner is a machine learning model that is only slightly better than random guessing. In gradient boosting, weak learners are typically decision trees. The idea is to combine many weak learners to create a strong learner.\n",
    "\n",
    "The gradient boosting algorithm works by iteratively fitting a new model to the residual errors of the previous model. The residual errors are the differences between the predicted values and the actual values. The new model is fit using a technique called gradient descent, which minimizes the loss function.\n",
    "\n",
    "The loss function is a measure of how well the model fits the data. The most common loss function for regression is the mean squared error (MSE). The MSE is the sum of the squared differences between the predicted values and the actual values.\n",
    "\n",
    "The gradient boosting algorithm starts by fitting a weak learner to the data. The residual errors are then calculated. A new weak learner is then fit to the residual errors. This process is repeated until the desired accuracy is achieved.\n",
    "\n",
    "The weak learners in gradient boosting are called weak because they are only slightly better than random guessing. However, by combining many weak learners, the gradient boosting algorithm can create a strong learner that is much more accurate than any individual weak learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7705d39c",
   "metadata": {},
   "source": [
    "The intuition behind the gradient boosting algorithm is to build an ensemble of weak learners that collectively learn to minimize a loss function. The weak learners are typically decision trees, and they are added sequentially to the model, each one trying to correct the errors of the previous models.The gradient boosting algorithm works by iteratively fitting a new model to the residual errors of the previous model. The residual errors are the differences between the predicted values and the actual values. The new model is fit using a technique called gradient descent, which minimizes the loss function.\n",
    "The loss function is a measure of how well the model fits the data. The most common loss function for regression is the mean squared error (MSE). The MSE is the sum of the squared differences between the predicted values and the actual values.\n",
    "The gradient boosting algorithm starts by fitting a weak learner to the data. The residual errors are then calculated. A new weak learner is then fit to the residual errors. This process is repeated until the desired accuracy is achieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43371650",
   "metadata": {},
   "source": [
    "The gradient boosting algorithm builds an ensemble of weak learners by iteratively fitting new models to the residual errors of the previous models. The residual errors are the differences between the predicted values and the actual values.\n",
    "\n",
    "The gradient boosting algorithm starts by fitting a weak learner to the data. The residual errors are then calculated. A new weak learner is then fit to the residual errors. This process is repeated until the desired accuracy is achieved.\n",
    "\n",
    "The gradient boosting algorithm can be summarized in the following steps:\n",
    "\n",
    "Initialize the predictions to the mean of the target values.\n",
    "For each iteration:\n",
    "Calculate the residual errors.\n",
    "Fit a new weak learner to the residual errors.\n",
    "Update the predictions by adding the predictions of the new weak learner.\n",
    "The predictions of the gradient boosting algorithm are the sum of the predictions of the individual weak learners. The weak learners are typically decision trees, but other models can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca78f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64c213",
   "metadata": {},
   "source": [
    "The mathematical intuition of the gradient boosting algorithm can be constructed in the following steps:\n",
    "Define a loss function that measures the error between the predicted values and the actual values. The most common loss function for regression is the mean squared error (MSE).\n",
    "Find the gradient of the loss function with respect to the predicted values. The gradient is a vector that points in the direction of the steepest descent.\n",
    "Update the predicted values by subtracting a small step in the direction of the gradient.\n",
    "Repeat steps 2 and 3 until the desired accuracy is achieved.\n",
    "The gradient boosting algorithm can be thought of as a way to minimize the loss function iteratively. By iteratively updating the predicted values in the direction of the gradient, the algorithm can gradually improve its fit to the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
