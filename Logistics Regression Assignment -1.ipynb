{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7839b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6da05",
   "metadata": {},
   "source": [
    "The key differences between linear regression and logistic regression models:\n",
    "\n",
    "Output: Linear regression models predict a continuous output, while logistic regression models predict a categorical output. For example, linear regression could be used to predict the price of a house, while logistic regression could be used to predict whether a patient has cancer or not.\n",
    "Relationship between the independent and dependent variables: Linear regression models assume a linear relationship between the independent and dependent variables. This means that the change in the dependent variable is proportional to the change in the independent variable. Logistic regression models do not assume a linear relationship between the independent and dependent variables.\n",
    "Learning algorithm: Linear regression models use a least squares algorithm to learn the parameters of the model. Logistic regression models use a maximum likelihood algorithm to learn the parameters of the model.\n",
    "Here is an example of a scenario where logistic regression would be more appropriate:\n",
    "\n",
    "Predicting whether a customer will click on an ad: The dependent variable in this case is whether or not the customer clicks on the ad, which is a categorical variable. Linear regression would not be appropriate because it assumes a linear relationship between the independent and dependent variables, which is not necessarily the case in this scenario. Logistic regression, on the other hand, does not assume a linear relationship, so it would be more appropriate for this task.\n",
    "Here are some other examples of tasks where logistic regression would be appropriate:\n",
    "\n",
    "Predicting whether a loan applicant will default on their loan\n",
    "Predicting whether a patient has a disease\n",
    "Predicting whether a customer will churn\n",
    "Predicting whether a student will pass an exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5675dc",
   "metadata": {},
   "source": [
    "\n",
    "The cost function used in logistic regression is the cross-entropy or log loss. It is a measure of how well the model's predictions match the actual labels. The lower the cross-entropy, the better the model's predictions.\n",
    "\n",
    "The cross-entropy is calculated as follows:\n",
    "\n",
    "H(y, h(x)) = -\\sum_{i=1}^{n} y_i \\log(h(x_i)) + (1 - y_i) \\log(1 - h(x_i))\n",
    "The cross-entropy is minimized using an iterative optimization algorithm, such as gradient descent. Gradient descent works by iteratively updating the model's parameters in the direction that reduces the cross-entropy.\n",
    "Here is an example of how the cross-entropy is calculated for a single data point with a true label of 1 and a predicted probability of 0.5:\n",
    "H(1, 0.5) = -1 \\log(0.5) - (1 - 1) \\log(1 - 0.5) = -1 \\log(0.5) = 1.386\n",
    "In this case, the cross-entropy is 1.386. This means that the model's prediction is not very good, since the true label is 1 and the predicted probability is 0.5.\n",
    "The cross-entropy is a very common cost function used in machine learning, especially for classification problems. It is a good measure of how well the model's predictions match the actual labels, and it can be minimized using efficient optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a7053",
   "metadata": {},
   "source": [
    "Regularization is a technique used to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data too well and is unable to generalize to new data. Regularization works by adding a penalty to the model's cost function that discourages the model from becoming too complex.\n",
    "There are two main types of regularization used in logistic regression:\n",
    "L1 regularization: This penalizes the sum of the absolute values of the model's coefficients. This encourages the model to have a smaller number of features with large coefficients, and can help to prevent the model from overfitting.\n",
    "L2 regularization: This penalizes the sum of the squares of the model's coefficients. This encourages the model to have all of its coefficients small, and can also help to prevent overfitting.\n",
    " regularization can be a very effective way to prevent overfitting in logistic regression. By adding a penalty to the model's cost function, regularization can encourage the model to have a simpler structure, which can lead to better generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dad58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cda2fc",
   "metadata": {},
   "source": [
    "A ROC curve, or Receiver Operating Characteristic curve, is a graphical plot of the true positive rate (TPR) against the false positive rate (FPR) for a binary classifier. The TPR is the ratio of true positives to the sum of true positives and false negatives, while the FPR is the ratio of false positives to the sum of false positives and true negatives.\n",
    "\n",
    "The ROC curve is a useful tool for evaluating the performance of a binary classifier because it shows the trade-off between TPR and FPR. A classifier with a high TPR will have a low FPR, but it may also have a high number of false positives. A classifier with a low FPR will have a high TPR, but it may also have a high number of false negatives.\n",
    "\n",
    "The ROC curve can be used to evaluate the performance of a logistic regression model by plotting the TPR and FPR for different values of the model's threshold. The threshold is the value at which the model predicts a positive class. A higher threshold means that the classifier is more likely to predict a positive class. This will result in a higher TPR, but it will also result in a higher FPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f56d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218340d8",
   "metadata": {},
   "source": [
    "Univariate selection: This involves selecting features one at a time, based on their statistical significance. A common approach is to use a p-value threshold to select features. Features with a p-value below the threshold are considered to be significant and are retained.\n",
    "Recursive feature elimination (RFE): This involves starting with all of the features and then iteratively removing the least important features one at a time. The importance of a feature is typically measured by its coefficient in the logistic regression model.\n",
    "Lasso regression: This is a type of regularization that penalizes the sum of the absolute values of the model's coefficients. This can help to prevent overfitting and can also be used to select features.\n",
    "Elastic net: This is a type of regularization that combines L1 and L2 regularization. This can be a more effective way to select features than L1 or L2 regularization alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1a4dc",
   "metadata": {},
   "source": [
    "versampling: This involves creating more data points for the minority class. This can be done by duplicating existing data points or by generating synthetic data points.\n",
    "Undersampling: This involves removing data points from the majority class. This can be done by randomly removing data points or by using a more sophisticated approach such as SMOTE.\n",
    "Cost-sensitive learning: This involves assigning different costs to misclassifications of different classes. This can help the model to focus on correctly classifying the minority class.\n",
    "Ensemble learning: This involves combining the predictions of multiple models. This can help to improve the overall performance of the model, especially when the models are trained on different subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f02680",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b46ff",
   "metadata": {},
   "source": [
    "Overfitting: This occurs when the model learns the training data too well and is unable to generalize to new data. This can be addressed by using regularization techniques, such as L1 or L2 regularization.\n",
    "Underfitting: This occurs when the model does not learn the training data well enough and is unable to make accurate predictions. This can be addressed by increasing the number of features or by using a more complex model.\n",
    "Multicollinearity: This occurs when there is a strong correlation between two or more independent variables. This can make it difficult for the model to learn the relationships between the independent variables and the dependent variable. This can be addressed by removing one of the correlated variables or by using a regularization technique such as ridge regression.\n",
    "Outliers: These are data points that are far away from the rest of the data. Outliers can can bias the model and make it less accurate. This can be addressed by removing outliers or by using a robust regression technique.\n",
    "Non-linearity: This occurs when the relationship between the independent variables and the dependent variable is not linear. This can make it difficult for the logistic regression model to make accurate predictions. This can be addressed by using a non-linear regression technique, such as decision trees or support vector machines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
