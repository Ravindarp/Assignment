{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fff553",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c406d",
   "metadata": {},
   "source": [
    "The relationship between polynomial functions and kernel functions in machine learning algorithms is that the polynomial kernel is a type of kernel function that uses a polynomial function to measure the similarity between two data points. The kernel function is a mathematical function that takes two data points as input and returns a value that measures their similarity. The polynomial kernel is defined as:\n",
    "K(x, y) = (x Â· y + c)^d\n",
    "where x and y are the data points, c is a constant, and d is the degree of the polynomial. The higher the degree of the polynomial, the more nonlinear the kernel function becomes.\n",
    "Polynomial functions are also used in machine learning algorithms, such as polynomial regression. Polynomial regression is a type of regression that uses a polynomial function to model the relationship between a dependent variable and one or more independent variables. The polynomial function is fit to the data using a least squares method.\n",
    "The relationship between polynomial functions and kernel functions in machine learning algorithms is that the polynomial kernel can be used to transform the data into a higher-dimensional space where the relationship between the data points is more linear. This allows machine learning algorithms to learn more accurate models of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eb7c865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "model=SVC(kernel='poly',degree=3)\n",
    "model.fit(x_train,y_train)\n",
    "pred=model.predict(x_test)\n",
    "print(accuracy_score(pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92751d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de2814",
   "metadata": {},
   "source": [
    "Increasing the value of epsilon in SVR will decrease the number of support vectors. This is because epsilon controls the width of the tube around the regression function. A larger epsilon means that the tube is wider, which means that fewer data points will be considered as outliers and will therefore be included as support vectors.\n",
    "In SVR, the goal is to find a function that approximates the relationship between the input variables and a continuous target variable, while minimizing the prediction error. The epsilon parameter controls the amount of error that is allowed. A larger epsilon means that more error is allowed, which means that fewer data points will be considered as outliers and will therefore be included as support vectors.\n",
    "Here is an example to illustrate this:\n",
    "Let's say we have a dataset of points that are scattered around a line. We can use SVR to fit a line to the data. The epsilon parameter controls the width of the tube around the line. If we set epsilon to a small value, then only the points that are very close to the line will be considered as support vectors. If we set epsilon to a large value, then more points will be considered as support vectors, even if they are not very close to the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f872496",
   "metadata": {},
   "source": [
    "Kernel function: The kernel function is a mathematical function that measures the similarity between two data points. The kernel function is used to transform the data into a higher-dimensional space where the relationship between the data points is more linear. This allows SVR to learn a more accurate model of the data.\n",
    "There are many different kernel functions that can be used with SVR, including the linear kernel, polynomial kernel, Gaussian kernel, and sigmoid kernel. The choice of kernel function depends on the nature of the data and the problem that you are trying to solve.\n",
    "For example, if the data is linearly separable, then the linear kernel can be used. If the data is not linearly separable, then a non-linear kernel such as the polynomial kernel or the Gaussian kernel can be used.\n",
    "C parameter: The C parameter controls the trade-off between the complexity of the model and its accuracy. A larger C value means that the model will be more complex and will have a lower training error, but it may also be more prone to overfitting. A smaller C value means that the model will be less complex and will have a higher training error, but it may be less prone to overfitting.\n",
    "The C parameter is important to tune when using SVR. A good way to tune the C parameter is to use cross-validation. Cross-validation is a technique that evaluates the performance of a model on a held-out set of data.\n",
    "Epsilon parameter: The epsilon parameter controls the amount of error that is allowed. A larger epsilon value means that more error is allowed, which means that the model will be less sensitive to outliers. A smaller epsilon value means that less error is allowed, which means that the model will be more sensitive to outliers.\n",
    "The epsilon parameter is also important to tune when using SVR. A good way to tune the epsilon parameter is to use cross-validation.\n",
    "Gamma parameter: The gamma parameter controls the smoothness of the decision boundary. A larger gamma value means that the decision boundary will be smoother. A smaller gamma value means that the decision boundary will be less smooth.\n",
    "The gamma parameter is not as important to tune as the C parameter and the epsilon parameter. However, it can be tuned to improve the performance of the model in some cases.\n",
    "Here are some examples of when you might want to increase or decrease the value of each parameter:\n",
    "Kernel function: If the data is linearly separable, then you can use the linear kernel. If the data is not linearly separable, then you can use a non-linear kernel such as the polynomial kernel or the Gaussian kernel.\n",
    "C parameter: If you are concerned about overfitting, then you can use a smaller C value. If you are not concerned about overfitting, then you can use a larger C value.\n",
    "Epsilon parameter: If you are concerned about outliers, then you can use a larger epsilon value. If you are not concerned about outliers, then you can use a smaller epsilon value.\n",
    "Gamma parameter: If you want a smoother decision boundary, then you can use a larger gamma value. If you want a less smooth decision boundary, then you can use a smaller gamma value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc97993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris=load_iris()\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "X_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "x_train_scaled=scaler.fit_transform(x_train)\n",
    "x_test_scaled=scaler.transform(x_test)\n",
    "svm=SVC()\n",
    "svm.fit(x_train_scaled,y_train)\n",
    "y_pred=svm.predict(x_test_scaled)\n",
    "accuracy=accuracy_score(y_pred,y_test) \n",
    "print(accuracy)\n",
    "param_grid={'C':[0.1,1,100],'kernel':['linear','rbf']}\n",
    "grid_search=GridSearchCV(SVC(),param_grid=param_grid,cv=3)\n",
    "grid_search.fit(x_train_scaled,y_train)\n",
    "tunded_classiier=grid_search.best_estimator_\n",
    "tunded_classiier\n",
    "import pickle\n",
    "with open('tuned_model.pkl','wb') as model_file:\n",
    "    pickel.dump(tuned_model,model_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97072119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c85cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
