{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fee9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2683d4",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure that indicates how much of the variation of a dependent variable is explained by an independent variable in a regression model. It is calculated as follows:\n",
    "R^2 = 1 - (SSR / SST)\n",
    "where:\n",
    "SSR is the sum of squared residuals, which is the sum of the squared distances between the actual values of the dependent variable and the predicted values from the regression model.\n",
    "SST is the total sum of squares, which is the sum of the squared distances between the actual values of the dependent variable and the mean of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c185e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda00743",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a statistic that measures the goodness of fit of a regression model, taking into account the number of independent variables in the model. It is calculated as follows:\n",
    "Adjusted R^2 = 1 - (SSR / (n - k - 1))\n",
    "where:\n",
    "SSR is the sum of squared residuals, as defined above.\n",
    "n is the number of observations in the dataset.\n",
    "k isThe main difference between adjusted R-squared and R-squared is that adjusted R-squared penalizes the model for adding additional independent variables. This is because R-squared can be inflated by adding more independent variables, even if those variables do not actually explain any of the variation in the dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef52ecc",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when the model has multiple independent variables. This is because adjusted R-squared penalizes the model for adding additional independent variables that do not significantly improve the fit of the model. This helps to prevent overfitting, which is a problem that can occur when the model is too complex and learns the noise in the data instead of the underlying relationships.\n",
    "Here are some specific situations where it is more appropriate to use adjusted R-squared:\n",
    "When you are comparing models with different numbers of independent variables.\n",
    "When you are concerned about overfitting the data.\n",
    "When you are using a regularization technique, such as ridge regression or LASSO regression.\n",
    "When the data is not normally distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afafcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d25233",
   "metadata": {},
   "source": [
    "RMSE\n",
    "Advantages:\n",
    "RMSE is easy to understand and interpret.\n",
    "It is a unitless metric, so it can be used to compare models with different dependent variables.\n",
    "It is not sensitive to outliers.\n",
    "Disadvantages:\n",
    "RMSE can be high if the dependent variable has a large variance.\n",
    "RMSE is not as sensitive to small errors as MAE.\n",
    "MSE\n",
    "Advantages:\n",
    "MSE is similar to RMSE, but it is more sensitive to outliers.\n",
    "MSE is a differentiable function, which means that it can be used to train machine learning models using gradient descent.\n",
    "Disadvantages:\n",
    "MSE is not unitless, so it cannot be used to compare models with different dependent variables.\n",
    "MSE is not as easy to understand and interpret as RMSE.\n",
    "MAE\n",
    "Advantages:\n",
    "MAE is less sensitive to outliers than RMSE.\n",
    "MAE is unitless, so it can be used to compare models with different dependent variables.\n",
    "Disadvantages:\n",
    "MAE is not as easy to understand and interpret as RMSE.\n",
    "MAE is not a differentiable function, so it cannot be used to train machine learning models using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abdfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0ec44",
   "metadata": {},
   "source": [
    "Lasso regularization is a technique used to prevent overfitting in linear regression models. It does this by adding a penalty to the loss function that is proportional to the sum of the absolute values of the coefficients. This penalty encourages some of the coefficients to be zero, which reduces the complexity of the model and makes it less likely to overfit the data\n",
    "e main difference between Lasso regularization and Ridge regularization is that Lasso regularization can shrink coefficients to zero, while Ridge regularization cannot. This makes Lasso regularization more effective for feature selection, as it can be used to identify the most important features in the model. However, Lasso regularization can also make the model less interpretable, as some of the coefficients may be zero.\n",
    "Lasso regularization is more appropriate to use when:\n",
    "You want to identify the most important features in the model.\n",
    "You are concerned about overfitting the data.\n",
    "You are willing to sacrifice some interpretability for a more accurate model.\n",
    "Ridge regularization is more appropriate to use when:\n",
    "You want to make the model more interpretable.\n",
    "You are not as concerned about overfitting the data.\n",
    "You need a model that is robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc464acb",
   "metadata": {},
   "source": [
    "Regularized linear models help to prevent overfitting in machine learning by adding a penalty to the loss function that discourages the model from becoming too complex. This penalty is typically proportional to the sum of the squared or absolute values of the coefficients, which means that it penalizes large coefficients more than small coefficients.\n",
    "As a result, regularized linear models tend to have smaller coefficients than non-regularized linear models. This makes them less sensitive to noise in the data and less likely to overfit the training data.\n",
    "Here is an example to illustrate how regularized linear models help to prevent overfitting. Let's say we have a dataset of house prices. We want to build a linear regression model to predict the price of a house based on its square footage.\n",
    "If we train a non-regularized linear regression model on this data, the model will likely fit the training data very well. However, the model may also overfit the training data, meaning that it will not generalize well to new data.\n",
    "This is because the non-regularized linear regression model will try to fit the noise in the training data as well as the underlying relationship between the square footage and the price of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06235857",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25063c1a",
   "metadata": {},
   "source": [
    "They can make the model less interpretable. Regularized linear models shrink the coefficients of the model, which can make it difficult to interpret the results. This is because the coefficients represent the importance of each feature in the model, and if the coefficients are too small, it can be difficult to tell which features are actually important.\n",
    "They can be computationally expensive to train. Regularized linear models require more computation to train than non-regularized linear models. This is because the regularizer adds a penalty to the loss function, which makes the optimization problem more difficult to solve.\n",
    "They may not be the best choice for all problems. Regularized linear models are not always the best choice for regression analysis. For example, if the data is not well-behaved, such as if it contains outliers or is not normally distributed, then regularized linear models may not be able to fit the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662bdb9",
   "metadata": {},
   "source": [
    "Here are some of the limitations of regularized linear models:\n",
    "They can make the model less interpretable. Regularized linear models shrink the coefficients of the model, which can make it difficult to interpret the results. This is because the coefficients represent the importance of each feature in the model, and if the coefficients are too small, it can be difficult to tell which features are actually important.\n",
    "They can be computationally expensive to train. Regularized linear models require more computation to train than non-regularized linear models. This is because the regularizer adds a penalty to the loss function, which makes the optimization problem more difficult to solve.\n",
    "They may not be the best choice for all problems. Regularized linear models are not always the best choice for regression analysis. For example, if the data is not well-behaved, such as if it contains outliers or is not normally distributed, then regularized linear models may not be able to fit the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f71ad",
   "metadata": {},
   "source": [
    "\n",
    "RMSE and MAE are both evaluation metrics used to measure the accuracy of regression models. RMSE stands for root mean squared error, while MAE stands for mean absolute error.\n",
    "\n",
    "RMSE is more sensitive to outliers than MAE. This means that a small number of large errors can have a big impact on the RMSE value. MAE is less sensitive to outliers, but it is also less sensitive to the overall trend of the data.\n",
    "\n",
    "In this case, Model A has a lower RMSE than Model B. This means that Model A is more accurate in terms of the average squared error between the predicted values and the actual values. However, Model B has a lower MAE than Model A. This means that Model B is more accurate in terms of the average absolute error between the predicted values and the actual values.\n",
    "\n",
    "Which model is the better performer depends on the specific application. If the application is sensitive to outliers, then Model A is the better performer. If the application is not sensitive to outliers, then Model B is the better performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ca347",
   "metadata": {},
   "source": [
    "Ridge regularization and Lasso regularization are both regularization methods that can be used to prevent overfitting in linear regression models. Ridge regularization adds a penalty to the loss function that is proportional to the sum of the squared values of the coefficients. Lasso regularization adds a penalty to the loss function that is proportional to the sum of the absolute values of the coefficients.\n",
    "\n",
    "The regularization parameter controls the amount of regularization that is applied. A higher regularization parameter will result in more regularization, which will make the model less complex and less likely to overfit.\n",
    "\n",
    "In this case, Model A uses Ridge regularization with a regularization parameter of 0.1. This means that Model A will be less regularized than Model B, which uses Lasso regularization with a regularization parameter of 0.5.\n",
    "\n",
    "This means that Model A is more likely to overfit than Model B. However, Model A is also more likely to preserve the interpretability of the model, as it is less likely to shrink coefficients to zero.\n",
    "\n",
    "Model B is less likely to overfit, but it is also more likely to remove important features from the model.\n",
    "\n",
    "Which model is the better performer depends on the specific application. If the application is sensitive to overfitting, then Model B is the better performer. If the application is more concerned with interpretability, then Model A is the better performer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
