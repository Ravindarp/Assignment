{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f80e0",
   "metadata": {},
   "source": [
    "Let's say there are 100 employees in the company.\n",
    "70 employees use the company's health insurance plan.\n",
    "28 of the employees who use the plan are smokers.\n",
    "The probability that an employee is a smoker given that he/she uses the health insurance plan is:\n",
    "P(Smoker|Uses Health Insurance) = 28/70 = 0.4\n",
    "This means that if an employee is randomly selected from the group of employees who use the health insurance plan, there is a 40% chance that he/she is a smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115f18e",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is the assumption that they make about the features.\n",
    "Bernoulli Naive Bayes assumes that the features are binary, meaning that they can only take on two values, such as 0 or 1. For example, a feature could be whether a word appears in a document or not.\n",
    "Multinomial Naive Bayes assumes that the features are categorical, meaning that they can take on any number of values. For example, a feature could be the number of times a word appears in a document.\n",
    "Bernoulli Naive Bayes is a simpler algorithm than Multinomial Naive Bayes, and it is less computationally expensive. However, it is also less flexible, and it can be less accurate for data with continuous features.\n",
    "Multinomial Naive Bayes is a more flexible algorithm, and it can be more accurate for data with continuous features. However, it is also more computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a35983",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da95e6",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by assuming that the missing value is equal to the most likely value for that feature. This is called the missing at random assumption.\n",
    "For example, if a feature is whether a word appears in a document, and the word is missing, then Bernoulli Naive Bayes will assume that the word is not present in the document.\n",
    "The missing at random assumption is not always true, but it is a reasonable assumption in many cases. If the missing values are not missing at random, then Bernoulli Naive Bayes may not be accurate.\n",
    "There are other ways to handle missing values in Bernoulli Naive Bayes, such as:\n",
    "Imputation: This is the process of filling in the missing values with some kind of estimate. For example, you could impute the missing values with the mean or median of the feature.\n",
    "Deletion: This is the process of deleting the instances with missing values.\n",
    "Modeling: This is the process of building a model to predict the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc4fa27",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a probabilistic classifier that assumes that the features are normally distributed. This means that the features are distributed in a bell-shaped curve.\n",
    "In multi-class classification, there are multiple classes that the data can be classified into. Gaussian Naive Bayes can be used to classify data into multiple classes by assuming that each class has its own normal distribution.\n",
    "The probability that a data point belongs to a particular class can be calculated by finding the probability that the data point's features come from the normal distribution of that class.\n",
    "Gaussian Naive Bayes is a simple and effective algorithm for multi-class classification. However, it is important to note that it assumes that the features are normally distributed. If the features are not normally distributed, then Gaussian Naive Bayes may not be accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b711b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e693f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB Metrics:\n",
      "Accuracy: 0.8839\n",
      "Precision: 0.8870\n",
      "Recall: 0.8152\n",
      "F1 Score: 0.8481\n",
      "\n",
      "MultinomialNB Metrics:\n",
      "Accuracy: 0.7863\n",
      "Precision: 0.7393\n",
      "Recall: 0.7215\n",
      "F1 Score: 0.7283\n",
      "\n",
      "GaussianNB Metrics:\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7104\n",
      "Recall: 0.9570\n",
      "F1 Score: 0.8131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = pd.read_csv(url, header=None, sep=',')\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "classifiers = [bernoulli_nb, multinomial_nb, gaussian_nb]\n",
    "metrics = {}\n",
    "for clf in classifiers:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    accuracy_scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(clf, X, y, cv=10, scoring='precision')\n",
    "    recall_scores = cross_val_score(clf, X, y, cv=10, scoring='recall')\n",
    "    f1_scores = cross_val_score(clf, X, y, cv=10, scoring='f1')\n",
    "    \n",
    "    metrics[clf_name] = {\n",
    "        \"Accuracy\": np.mean(accuracy_scores),\n",
    "        \"Precision\": np.mean(precision_scores),\n",
    "        \"Recall\": np.mean(recall_scores),\n",
    "        \"F1 Score\": np.mean(f1_scores)\n",
    "    }\n",
    "for clf_name, metric_values in metrics.items():\n",
    "    print(f\"{clf_name} Metrics:\")\n",
    "    for metric, value in metric_values.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c6f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
