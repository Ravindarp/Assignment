{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f765711",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbed1a6",
   "metadata": {},
   "source": [
    "Grid Search CV (Grid Search Cross-Validation) is a technique used in machine learning to search and find the optimal combination of hyperparameters for a given model. Hyperparameters are the parameters of a machine learning model that are not learned from the data. They are typically set by the user, and their values can have a significant impact on the performance of the model.\n",
    "Grid Search CV works by first creating a grid of possible hyperparameter values. For each hyperparameter, you specify a range of possible values to try. Grid Search CV then evaluates each combination of hyperparameter values using cross-validation. Cross-validation is a technique for evaluating the performance of a machine learning model by dividing the data into multiple folds. The model is trained on a subset of the data (called the training set) and then evaluated on the remaining data (called the test set). This process is repeated multiple times, with each fold used as the test set once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e144f1",
   "metadata": {},
   "source": [
    "Grid Search CV and Randomized Search CV are both techniques used in machine learning to search for the optimal combination of hyperparameters for a given model. However, they have some key differences.\n",
    "Grid Search CV exhaustively searches through all possible combinations of hyperparameter values. This means that it will try every possible combination of the values you specify for each hyperparameter. This can be computationally expensive, especially if the grid of possible hyperparameter values is large.\n",
    "Randomized Search CV randomly samples a subset of the possible combinations of hyperparameter values. This means that it will not try every possible combination, but instead it will select a random sample of combinations. This can be much faster than Grid Search CV, especially if the grid of possible hyperparameter values is large.\n",
    "In general, Grid Search CV is a more thorough approach to hyperparameter tuning, while Randomized Search CV is a faster approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4378166",
   "metadata": {},
   "source": [
    "Data leakage is a problem in machine learning that occurs when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the model being constructed.\n",
    "There are two main types of data leakage:\n",
    "Target leakage: This occurs when the target variable, or the variable that you are trying to predict, is present in the training data. For example, if you are trying to predict whether a customer will default on their loan, then the target variable would be the customer's credit score. If the credit score is present in the training data, then the model can simply memorize the credit scores and predict that all customers with a certain credit score will default on their loan. This will lead to the model overfitting the training data and performing poorly on new data.\n",
    "Train-test contamination: This occurs when data from the test set is accidentally used to train the model. This can happen if the data sets are not properly separated, or if the model is trained and evaluated using the same data. Train-test contamination can lead to the model overfitting the training data and performing poorly on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e576afb",
   "metadata": {},
   "source": [
    "There are a number of ways to prevent data leakage when building a machine learning model. Here are some of the most common:\n",
    "Carefully separate the training data from the test data. The training data is used to train the model, and the test data is used to evaluate the model. It is important to make sure that the training data and the test data are not mixed up.\n",
    "Use a holdout set to evaluate the model. A holdout set is a set of data that is kept separate from the training data and the test data. The holdout set is used to evaluate the model after it has been trained. This helps to ensure that the model is not overfitting the training data.\n",
    "Use a technique called cross-validation. Cross-validation is a technique that uses multiple folds of the data to evaluate the model. This helps to ensure that the model is not overfitting the data.\n",
    "Use a technique called data anonymization. Data anonymization is a technique that removes identifying information from the data. This helps to prevent the model from learning information that it should not know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93632f",
   "metadata": {},
   "source": [
    "confusion matrix is a table that is used to summarize the performance of a classification model. It is a performance evaluation tool in machine learning, representing the accuracy of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives produced by the model on the test data. For binary classification, the matrix will be of a 2X2 table, For multi-class classification, the matrix shape will be equal to the number of classes i.e for n classes it will be nXn.\n",
    "The confusion matrix is a useful tool for understanding how well a classification model is performing. It can be used to identify the types of errors that the model is making, and to see how the model is performing for different classes.\n",
    "The confusion matrix is divided into four quadrants:\n",
    "True Positives (TP): These are the cases where the model correctly predicted the positive class.\n",
    "True Negatives (TN): These are the cases where the model correctly predicted the negative class.\n",
    "False Positives (FP): These are the cases where the model incorrectly predicted the positive class.\n",
    "False Negatives (FN): These are the cases where the model incorrectly predicted the negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef9ba1",
   "metadata": {},
   "source": [
    "Precision and recall are two metrics used to evaluate the performance of a classification model in the context of a confusion matrix.\n",
    "Precision is the fraction of positive predictions that are actually positive. It is calculated as follows:\n",
    "Precision = TP / (TP + FP)\n",
    "where TP is the number of true positives and FP is the number of false positives.\n",
    "Recall is the fraction of positive instances that are correctly predicted as positive. It is calculated as follows:\n",
    "Recall = TP / (TP + FN)\n",
    "where FN is the number of false negatives.\n",
    "In other words, precision measures how accurate the model is, while recall measures how complete the model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34984f",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is used to summarize the performance of a classification model. It is a performance evaluation tool in machine learning, representing the accuracy of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives produced by the model on the test data. For binary classification, the matrix will be of a 2X2 table, For multi-class classification, the matrix shape will be equal to the number of classes i.e for n classes it will be nXn.\n",
    "The confusion matrix can be used to interpret which types of errors your model is making. The four quadrants of the confusion matrix are:\n",
    "True Positives (TP): These are the cases where the model correctly predicted the positive class.\n",
    "True Negatives (TN): These are the cases where the model correctly predicted the negative class.\n",
    "False Positives (FP): These are the cases where the model incorrectly predicted the positive class.\n",
    "False Negatives (FN): These are the cases where the model incorrectly predicted the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8c4bf",
   "metadata": {},
   "source": [
    "There are many common metrics that can be derived from a confusion matrix, each with its own advantages and disadvantages. Some of the most common metrics include:\n",
    "Accuracy: This is the most common metric and is calculated as the number of correct predictions divided by the total number of predictions.\n",
    "Precision: This metric measures the fraction of positive predictions that are actually positive.\n",
    "Recall: This metric measures the fraction of positive instances that are correctly predicted as positive.\n",
    "F1 Score: This metric is a weighted average of precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4dfcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e42dc",
   "metadata": {},
   "source": [
    "The accuracy of a model is the fraction of predictions that the model gets correct. It is calculated by dividing the number of correct predictions by the total number of predictions.\n",
    "The values in the confusion matrix represent the number of correct predictions and incorrect predictions.\n",
    "True positives are the number of predictions that are correct.\n",
    "True negatives are the number of predictions that are incorrect.\n",
    "False positives are the number of predictions that are incorrect but should have been correct.\n",
    "False negatives are the number of predictions that are incorrect but should have been incorrect.\n",
    "The accuracy of a model is related to the values in the confusion matrix as follows:\n",
    "The accuracy of a model is equal to the sum of the true positives and true negatives divided by the total number of predictions.\n",
    "The accuracy of a model can be increased by increasing the number of true positives and true negatives, and by decreasing the number of false positives and false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd844d",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is used to summarize the performance of a classification model. It is a performance evaluation tool in machine learning, representing the accuracy of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives produced by the model on the test data. For binary classification, the matrix will be of a 2X2 table, For multi-class classification, the matrix shape will be equal to the number of classes i.e for n classes it will be nXn.\n",
    "The confusion matrix can be used to identify potential biases or limitations in your machine learning model by looking at the number of false positives and false negatives.\n",
    "False positives are instances where the model incorrectly predicts the positive class. For example, if a model is used to detect spam emails, a false positive would be an email that is actually not spam but is predicted by the model to be spam. False positives can be a sign of bias if the model is more likely to predict the positive class for certain types of instances. For example, a model that is trained on a dataset of spam emails that is mostly from men might be more likely to predict spam for emails that are from men.\n",
    "False negatives are instances where the model incorrectly predicts the negative class. For example, if a model is used to detect spam emails, a false negative would be an email that is actually spam but is predicted by the model to be not spam. False negatives can be a sign of limitation if the model is not able to correctly identify certain types of instances. For example, a model that is trained on a dataset of spam emails that is mostly from English speakers might not be able to correctly identify spam emails that are written in other languages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
