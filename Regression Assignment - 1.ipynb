{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e39614",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ef17f",
   "metadata": {},
   "source": [
    "The main difference between simple linear regression and multiple linear regression is the number of independent variables. Simple linear regression has only one independent variable, while multiple linear regression has two or more independent variables.\n",
    "\n",
    "In simple linear regression, the relationship between the dependent variable and the independent variable is modeled by a straight line. The equation of this line is:\n",
    "\n",
    "y = mx + b\n",
    "Multiple linear regression allows us to model the relationship between the dependent variable and multiple independent variables. The equation of a multiple linear regression model is:\n",
    "\n",
    "y = mx1 + bx2 + cx3 + ... + dn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73891ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562553b2",
   "metadata": {},
   "source": [
    "Linearity: The relationship between the dependent variable and the independent variable is linear. This means that the residuals (the difference between the actual values of the dependent variable and the predicted values) should be randomly scattered around the line of best fit.\n",
    "Homoscedasticity: The variance of the residuals should be constant across all values of the independent variable. This means that the residuals should be equally spread out around the line of best fit, regardless of the value of the independent variable.\n",
    "Normality: The residuals should be normally distributed. This means that the residuals should be bell-shaped, with most of the values clustered around the mean and fewer values towards the tails of the distribution.\n",
    "Independence: The residuals should be independent of each other. This means that the residuals from one observation should not be correlated with the residuals from any other observation.\n",
    "Multicollinearity: The independent variables should not be highly correlated with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6427e",
   "metadata": {},
   "source": [
    "The slope and intercept are the two coefficients in a linear regression model. The slope represents the change in the dependent variable for a one-unit change in the independent variable. The intercept represents the value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "For example, let's say we are trying to predict the price of a house based on its square footage. The slope would tell us how much the price of the house increases for every additional square foot. The intercept would tell us the price of a house with zero square footage, which is not realistic but is necessary to have a complete linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a754e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa79164",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative optimization algorithm used to find the minimum of a function. It works by starting at a point and then moving in the direction of the steepest descent until it reaches a minimum.\n",
    "\n",
    "In machine learning, gradient descent is used to train machine learning models. The goal of machine learning is to find the parameters of a model that minimize a cost function. The cost function measures how well the model fits the data. Gradient descent is used to find the parameters that minimize the cost function.\n",
    "\n",
    "There are two main types of gradient descent: batch gradient descent and stochastic gradient descent. Batch gradient descent calculates the gradient of the cost function over the entire training set. Stochastic gradient descent calculates the gradient of the cost function over one training example at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf844b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15496d",
   "metadata": {},
   "source": [
    "Multiple linear regression is a statistical model that predicts a continuous dependent variable from one or more independent variables. The model is linear in the parameters, meaning that the predicted value is a linear combination of the independent variables.\n",
    "\n",
    "The equation of a multiple linear regression model is:\n",
    "\n",
    "y = mx1 + bx2 + cx3 + ... + dn\n",
    "where y is the dependent variable, x1, x2, x3, ..., xn are the independent variables, and m, b, c, ..., d are the coefficients of the independent variables.\n",
    "\n",
    "In simple linear regression, there is only one independent variable. In multiple linear regression, there are two or more independent variables. This allows the model to capture more complex relationships between the dependent variable and the independent variables.\n",
    "The main difference between simple linear regression and multiple linear regression is the number of independent variables. Simple linear regression can only model a linear relationship between one dependent variable and one independent variable. Multiple linear regression can model a linear relationship between one dependent variable and two or more independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72517fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cfcc12",
   "metadata": {},
   "source": [
    "Multicollinearity is a problem that can occur in multiple linear regression when two or more independent variables are highly correlated with each other. This means that the independent variables are essentially measuring the same thing, so they cannot provide unique information about the dependent variable.\n",
    "\n",
    "Here is an example. Let's say we are trying to predict the price of a house based on its square footage and number of bedrooms. If the square footage and number of bedrooms are highly correlated, then they are essentially measuring the same thing. In this case, the multiple linear regression model will not be able to accurately predict the price of the house.\n",
    "\n",
    "There are a few ways to detect multicollinearity. One way is to look at the correlation matrix of the independent variables. If two or more independent variables have a correlation coefficient that is close to 1, then they are likely to be collinear.\n",
    "\n",
    "Another way to detect multicollinearity is to use the variance inflation factor (VIF). The VIF is a measure of how much the variance of a coefficient is inflated due to multicollinearity. A VIF greater than 10 indicates that there is a high degree of multicollinearity between the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221cd51",
   "metadata": {},
   "source": [
    "Polynomial regression is a statistical model that predicts a continuous dependent variable from a set of independent variables. The model is linear in the parameters, but the relationship between the dependent variable and the independent variables is modeled using a polynomial function.\n",
    "\n",
    "The equation of a polynomial regression model is:\n",
    "\n",
    "y = ax^n + bx^(n-1) + cx^(n-2) + ... + dx + e\n",
    "where y is the dependent variable, x is the independent variable, a, b, c, ..., d, and e are the coefficients of the polynomial function.\n",
    "\n",
    "In linear regression, the relationship between the dependent variable and the independent variable is modeled using a linear function. This means that the degree of the polynomial is 1. In polynomial regression, the degree of the polynomial can be any positive integer.\n",
    "\n",
    "For example, let's say we are trying to predict the price of a house based on its square footage. In linear regression, we would use a model of the form:\n",
    "\n",
    "y = mx + b\n",
    "where m is the slope of the line and b is the y-intercept. In polynomial regression, we could use a model of the form:\n",
    "\n",
    "y = ax^2 + bx + c\n",
    "where a, b, and c are the coefficients of the polynomial function. This model would allow us to capture the non-linear relationship between the price of the house and its square footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05dfd8c",
   "metadata": {},
   "source": [
    "Advantages of polynomial regression over linear regression:\n",
    "\n",
    "Can model more complex relationships between the dependent and independent variables.\n",
    "Can be used to fit data that is not well-modeled by a linear function.\n",
    "Can be used to improve the accuracy of predictions.\n",
    "Disadvantages of polynomial regression over linear regression:\n",
    "\n",
    "More complex to fit and interpret.\n",
    "More sensitive to outliers.\n",
    "More prone to overfitting.\n",
    "When to use polynomial regression:\n",
    "\n",
    "Polynomial regression should be used when the relationship between the dependent and independent variables is not linear. For example, if the relationship is quadratic or cubic, then polynomial regression can be used to model the relationship.\n",
    "\n",
    "Polynomial regression can also be used to improve the accuracy of predictions when the linear regression model is not accurate enough. However, it is important to be careful not to overfit the model, as this can lead to inaccurate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
